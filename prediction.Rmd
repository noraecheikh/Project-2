# Prediction Project
> Group 5
> by: Derek Fine, Dexter Offer, Matt Webber, Mitchell Stephens, and Nora Cheikh

## Section 1: Executive Summary and Documentation of Preprocessing
This project includes five proposed machine learning models for classification predictions for body movement detection data. We all started with the same clean datasets and created individual models. We varied the type of model, random samples, and variables used. We chose the {insert model} because {insert reasons}.

### Cleaning Process
#### Pull in Raw Datasets
> Data was downloaded onto desktop from: https://github.com/slevkoff/ECON386REPO/tree/master/Prediction%20Project
> First, the csv files were loaded into R.
> The raw training dataset has 160 variables and 19,622 observations.
> The raw testing dataset has 160 variables and 20 observations.
```{r} 
setwd("~/Desktop") #set the working directory
filename <- "testing.csv" #create label for filename
Testing <- read.csv(filename) #pull csv data frame into RStudio
filename <- "training.csv" #create label for filename
Training <- read.csv(filename) #pull csv data frame into RStudio
```
#### Removal of NAs, Empty, and Unnecessary Columns
> Some of the variables were removed because all or most of the values were NA or empty vectors. 
```{r}
testing1 <- Filter(function(x)!all(is.na(x) || is.null(x) || x == "" || x == 0), Testing) #remove columns that are empty or have NAs
testing1$problem_id <-NULL #remove the "problem_id" column from testing data
training1 <- Training[ , colSums(is.na(Training)) == 0]
testing1names <- colnames(testing1)
write.table(testing1names, file="testingnames.txt")
training1names <- colnames(training1)
write.table(training1names, file="training1names.txt")
#compare the column names
training2 <- training1[ -c(12:20,43:48,52:60,74:82) ] #manually remove the columns that are in training1 but not in testing1 and label new data frame training2
```
#### Save the Clean Datasets
> The new datasets should preserve the original number of observations and change the number of variables in the training set to 60 and in the testing set to 59.
```{r}
write.csv(training2, file="cleantraining.csv", row.names = FALSE)
write.csv(testing1, file="cleantesting.csv", row.names = FALSE)
```

## Section 2: Model Proposed by Matt
### Load the necessary libraries
```{r}
library(randomForest)
library(confusionMatrix)
```
### Pull in the clean data
```{r}
cleantraining<-read.csv("cleantraining.csv")
cleantesting<-read.csv("cleantesting.csv")
```
### Set the levels of the training and testing data equal for the cvtd_timestamp and new_window variables
```{r}
levels(cleantesting$cvtd_timestamp) <- levels(cleantraining$cvtd_timestamp)
levels(cleantesting$new_window) <- levels(cleantraining$new_window) 
```

### Partition the training data to build the model
```{r}
set.seed(1234)
trainingRowIndex<-sample(1:nrow(cleantraining), size = .8*nrow(cleantraining))
trainingData<-cleantraining[trainingRowIndex, ]
testData <-cleantraining[-trainingRowIndex, ]
```

### Create a Random Forest model using all variables except X
```{r}
Matt1<-randomForest(classe~.-X, trainingData, ntree=50, norm.votes=F)
```
### Test/Validate Matt1 model using the partitioned testing data (testData)
```{r}
pred_Matt<-predict(Matt1, testData)
confusionMatrix(pred_Matt, testData$classe)
```
**Results of the confusionMatrix**

Statistic|Value
---------|-----
Accuracy|0.9992
95% CI|0.9978, 0.9998
P-Value|<2.2e-16

Prediction|A|B|C|D|E
----------|-|-|-|-|-
A|1114|0|0|0|0
B|0|721|0|0|0
C|0|0|689|2|0
D|0|0|0|663|1
E|0|0|0|0|705

### Predict on out-of-sample testing data (cleantesting)
```{r}
guess_Matt<-predict(Matt1, cleantesting)
```
**Prediction Results**

Problem_id |1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20
-----------|-|-|-|-|-|-|-|-|-|--|--|--|--|--|--|--|--|--|--|--
classe |B|A|B|A|A|E|D|B|A|A|B|C|B|A|E|E|A|B|B|B

## Section 3: Model Proposed by Dexter Offer
### Load The Clean Datasets
```{r}
cleantraining <- read.csv("cleantraining.csv") #load the clean training set
cleantesting <- read.csv("cleantesting.csv") #load the clean testing set
```
### Fix the Levels of the Testing Set
> The levels of the testing set do not match the levels of the training set.This is because the testing set only contains 20 observations, so the levels do not vary as much.
```{r}
levels.time <- levels(cleantraining$cvtd_timestamp) #create vector of the levels of the cvtd_timestamp
levels(cleantesting$cvtd_timestamp) <- levels.time #change the levels of cvtd_timestamp in the testing set to match the training set
levels.window <- levels(cleantraining$new_window) #do the same for the new_window variable
levels(cleantesting$new_window) <- levels.window
```
### Partition the In-Sample Testing and Training Sets
> Partition the *cleantraining* data into two sections: 70% for training the model and 30% to test how well the model predicts and to not overfit the data.
```{r}
 set.seed(1234) #random number generator; randomize selection of the sample subsets
 trainingRowIndex<-sample(1:nrow(cleantraining), size = .7*nrow(cleantraining)) 
 #partition 70% of the data for the training, 30% of the data for testing
 trainingData1<-cleantraining[trainingRowIndex, ] #create training set
 testData1 <-cleantraining[-trainingRowIndex, ] #create testing set
```
### Random Forest Model
> The randomForest function in R implements Breiman's random forest algorithm (based on Breiman and Cutler's original Fortran code) for classification and regression. 
> I used all of the variables in the dataset, except for the *X* , *raw_timestamp_part_2* , *raw_timestamp_part_1* *cvtd_timestamp* variables. *X* is an id variable andTimestamp variables I determined to be unrelated to the class variable. This model uses 500 trees with the importance and proximity argument both set equal to TRUE.
> The randomForest function creates its own partition of the data and checks the out of sample error - Out of Bag Error (OOB) - of the estimation. This is printed out after running the function along with the number of trees and the number of variables tired at each split. 

```{r}
install.packages("randomForest") #Install Random Forest package
library(randomForest) #Load package
print(head(trainingData1)) #View data sample
RFModel <- randomForest(classe~. -X -raw_timestamp_part_1 -raw_timestamp_part_2 -cvtd_timestamp, data = trainingData1, ntree=500, importance=TRUE, proximity=TRUE)

Number of trees: 500
No. of variables tried at each split: 7

        OOB estimate of  error rate: 0.28%
        
        
```

### Test/Cross-Validate Model
> To evaluate how acurately my model is predicting the dependet classification variable I used the "caret" package in R, Using the predict()
> To test how well the model predicted the in-sample testing data, I used a confusion matrix. 
```{r}
install.packages(caret) #for the confusion matrix
library(caret)
predictRFModel <- predict(RFModel, testData1) #creates prediction of classe with the in-sample testing data
      Confusion Matrix and Statistics
confusionMatrix(predictRFModel,testData1$classe) #table to describe the perfomance of the classification model
```

Results of the Confusion Matrix
 | 
----------|--------
Accuracy | 99.71% 
95% Confidence Interval | (0.9954, 0.9983)
P-value | <2.2e-16
### Predict The Out-of-Sample Classifications
> The prediction of the the out-of-sample dataset class.
```{r}
predict <- predict(RFModel, cleantesting)
```
### Predictions for Out-of-Sample Classification
Problem_id | 1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20
------- | -|-|-|-|-|-|-|-|-|--|--|--|--|--|--|--|--|--|--|--
__classe__|B|A|B|A|A|E|D|B|A|A|B|C|B|A|E|E|A|B|B|B


## Section 4: Model Proposed by

## Section 5: Model Proposed by 

## Section 6: Model Proposed by Nora Cheikh
### Load The Clean Datasets
```{r}
cleantraining <- read.csv("cleantraining.csv") #load the clean training set
cleantesting <- read.csv("cleantesting.csv") #load the clean testing set
```
### Fix the Levels of the Testing Set
> The levels of the testing set do not match the levels of the training set because the testing set only has 20 observations, so the levels do not vary as much.
```{r}
levels.time <- levels(cleantraining$cvtd_timestamp) #create vector of the levels of the cvtd_timestamp
levels(cleantesting$cvtd_timestamp) <- levels.time #change the levels of cvtd_timestamp in the testing set to match the training set
levels.window <- levels(cleantraining$new_window) #do the same for the new_window variable
levels(cleantesting$new_window) <- levels.window
```
### Partition the In-Sample Testing and Training Sets
> Partition the *cleantraining* data into two sections: 70% for training the model and 30% to test how well the model predicts and to not overfit the data.
```{r}
 set.seed(1234) #random number generator; randomize selection of the sample subsets
 trainingRowIndex<-sample(1:nrow(cleantraining), size = .7*nrow(cleantraining)) 
 #partition 70% of the data for the training, 30% of the data for testing
 trainingData1<-cleantraining[trainingRowIndex, ] #create training set
 testData1 <-cleantraining[-trainingRowIndex, ] #create testing set
```
### Support Vector Machine Model
> A support vector machine is a supervised learning model that can be used for classification. After researching other studies that predicted movement data, I found the support vector machine to be a popular choice of model for this particular type of data classification. I used the *e1071* R package to estimate the model.
> I used all of the variables, except for the *X* and the *new_window* variables. *X* is an id variable and *new_window* indicates a change in window, which all values were 'no' in the testing data, so it is not informative. 
> To find the optimal parameters, I tuned the model using command {best.svm()} that did a grid search which creates a set of models and cross-validates to find the best model. The cost is a tuning parameter for C-classification; it weights for the soft margin. I used a radial basis (RBF) function to get the best predictive performance based on the high number of variables in this dataset.
> The model has 2976 support vectors.
```{r}
install.packages(e1071) #for the support vector machine (svm) commands 
library(e1071)
model.nc <- best.svm(classe~.-X -new_window, data = trainingData1, cost = 2^(2:8),  kernel = "radial")
```

### Test/Cross-Validate Model
> The evaluation of how well this model is at predicting the classe classifications. I used the *caret* R package to cross-validate the model. Using the predict() command, I used the model to predict the in-sample testing data.
> To test how well the model predicted the in-sample testing data, I used a confusion matrix. A confusion matrix is a table that describes the performance of the classification test. Results are shown below.
```{r}
install.packages(caret) #for the confusion matrix
library(caret)
predict1 <- predict(model.nc, testData1) #creates prediction of classe with the in-sample testing data
confusionMatrix(predict1,testData1$classe)
#table to describe the perfomance of the classification model
```
Results of the Confusion Matrix
 | 
----------|--------
Accuracy | 99.49%
95% Confidence Interval | ( .9927, .9966)
P-value | <2.2e-16
### Predict The Out-of-Sample Classifications
> The prediction of the the out-of-sample dataset class.
```{r}
predict <- predict(model.nc, cleantesting)
```
### Predictions for Out-of-Sample Classification
Problem_id | 1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20
------- | -|-|-|-|-|-|-|-|-|--|--|--|--|--|--|--|--|--|--|--
__classe__|B|A|B|A|A|E|D|B|A|A|B|C|B|A|E|E|A|B|B|B


## Section 7: Final Proposal and Discussion 
We chose {insert model} as our final model because {reasons}. This model had ###.

### Predictions by Model
Problem_id | 1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20
------- | -|-|-|-|-|-|-|-|-|--|--|--|--|--|--|--|--|--|--|--
__classe__|B|A|B|A|A|E|D|B|A|A|B|C|B|A|E|E|A|B|B|B
